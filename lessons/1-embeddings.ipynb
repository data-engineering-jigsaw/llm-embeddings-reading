{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07681935-6080-4918-a07e-9784aacbfe25",
   "metadata": {},
   "source": [
    "# Using a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6430107b-45fd-447e-ab49-9fa6f8db67ca",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43340a01-9c98-4842-b235-a3a30e50edee",
   "metadata": {},
   "source": [
    "Now so far, we have seen one technique for adding information to our llm application.  The technique was to get a question then use google to search for information and present that to our model to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b37d3a-1e5b-4bc4-acb9-def891d0ff81",
   "metadata": {},
   "source": [
    "<img src=\"./question-answer.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1173c4b-1d36-4ae1-960f-b946a3351932",
   "metadata": {},
   "source": [
    "Now above, we used a google search to provide this information.  But oftentimes we cannot use Google to search for this information to provide to our llm.  \n",
    "\n",
    "> **Why not**?  Well, for one, the information may be private, like if we want to search through internal company documents.  Or, we may have to first perform pre-processing work, like selecting just the correct text to present to our llm.  Or we may need to ensure a level of data accuracy before feeding that data to our LLM.\n",
    "\n",
    "So instead of using Google, we'll need to search for this information in a database.  \n",
    "\n",
    "In that case, our architecture will look like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce53c1a-5f1d-48b3-83d8-e628a1b1206e",
   "metadata": {},
   "source": [
    "<img src=\"./rag-pipeline.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc50ddb-eb24-43c2-9d7d-3013e7e3c05a",
   "metadata": {},
   "source": [
    "So in this case, we are not searching Google for content to then send to our LLM.  Instead, we are searching our database for the relevant content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5b8906-71ac-49f9-9edb-6800cc9f036e",
   "metadata": {},
   "source": [
    "The hard part, of course, is that instead of a SQL statement -- our query is text like \"Was barbie nominated for an Oscar\".  And of course, the content in our database is text as well.  So how do we take a text-based query, and retrieve the relevant content from our database?  \n",
    "\n",
    "That's what we'll discuss next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b507882-5e96-409d-9296-8b9abdea21c9",
   "metadata": {},
   "source": [
    "### Text to vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a6358-0bae-46ac-9007-f76228bfdb08",
   "metadata": {},
   "source": [
    "The key thing is to convert our content in the database into a numerical representation, and also to turn our question into a numerical representation.  These numerical representations are called embeddings.  \n",
    "\n",
    "An embedding is a vector that represents text.  For example, the word queen may be represented by the following vector:\n",
    "\n",
    "`[1.17129, 2.12989, 3.1980128]`\n",
    "\n",
    "When someone asks a question, we'll turn the question into a vector, and return the vectors in our database most similar to the query vector.\n",
    "\n",
    "For example, in the `example_1.py` file, you'll see some initial lines of code that takes text and turns each word into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13d508c8-2330-4f21-94cc-4107e2bb6bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "api_key = \"\"\n",
    "client = OpenAI(\n",
    "    api_key=api_key\n",
    ")  # get API key from platform.openai.com\n",
    "\n",
    "MODEL = \"text-embedding-3-small\"\n",
    "inputs = [\n",
    "        \"Tree\", \"Bagel\", \"Software Developer\", \"King\", \"Queen\", \"Prince\"\n",
    "    ]\n",
    "res = client.embeddings.create(\n",
    "    input=inputs, model=MODEL\n",
    ")\n",
    "df = pd.DataFrame(inputs, columns = ['text'])\n",
    "vectors = res.data\n",
    "\n",
    "np_embeddings = [np.array(vector.embedding) for vector in vectors]\n",
    "df = df.assign(embedding = np_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a5b3c-cdc7-48cb-b56a-15311ddf1614",
   "metadata": {},
   "source": [
    "Then if someone gives us a question, we convert that text to a vector, and compare to the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5db886-2fce-4992-afec-ec2307591ef1",
   "metadata": {},
   "source": [
    "> So below, you can imagine our question text is `King`.  We convert that to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16ab6e3-d139-46cf-94cc-94d687bf49a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question = \"King\"\n",
    "# q_embeddings = client.embeddings.create(input=question, model=MODEL)\n",
    "# q_embedding_array = np.array(q_embeddings.data[0].embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57e5c7-593f-4d2f-8d45-44fa01f1f6bd",
   "metadata": {},
   "source": [
    "Then the cosine function compares the similarity between the our question vector (representing king), and each of the other words above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b445cb0-f64c-423f-bf39-89a177c860f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [cosine(q_embedding_array, embedding) for embedding in embeddings]\n",
    "df['distances'] = distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c8282-06b8-4f98-b251-146f9df6c1d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then we can sort the dataframe by the words that are most similar to our vector question vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9443a02c-ec7f-4dc8-aa07-d70957de1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('distances')[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc05ad-3aa1-4fd2-b87d-986601eabeb6",
   "metadata": {},
   "source": [
    "We get the text from those closes three entries and send it to our ai model as the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc85bf-81dc-4540-b38a-9d0fddb13215",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '\\n\\n'.join(df.sort_values('distances')[:3].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac048aa-b0a9-45df-947d-0f2b974e563a",
   "metadata": {},
   "source": [
    "So this way, instead of our ai model getting information from the internet, we find the relevant information from our database, and that becomes the context that we send to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3821dcf-c0af-447a-9f73-dde49e91c779",
   "metadata": {},
   "source": [
    "<img src=\"./rag-pipeline.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dcc318-3908-45bc-a484-807808415017",
   "metadata": {},
   "source": [
    "This pattern is called a rag pipeline.  Rag stands for retrieval augmented generation.  This is because the text generation from our llm is *augmented* by retrieving data from the database, and presenting it to our llm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d7610-64bc-4094-8689-1505c0a186da",
   "metadata": {},
   "source": [
    "> You can see another example in the `example_2.py` file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454132a6-9dee-4175-8f95-cc131bf147d4",
   "metadata": {},
   "source": [
    "[Pinecone OpenAI](https://docs.pinecone.io/docs/openai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e55acc9-66b4-4395-9095-722546097c6c",
   "metadata": {},
   "source": [
    "[Pinecone Quickstart](https://docs.pinecone.io/docs/quickstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfed335-3c24-471c-9e0c-07717d76e5f4",
   "metadata": {},
   "source": [
    "[Text to Vector Lesson](https://github.com/jigsawlabs-student/pytorch-nlp/blob/master/2-embedding-words.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c284cd-0a74-409a-9a98-2acf57ff055c",
   "metadata": {},
   "source": [
    "[Hacker News Who is Hiring](https://marcotm.com/articles/information-extraction-with-large-language-models-parsing-unstructured-data-with-gpt/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734a4fe-d3eb-4d0c-a247-6e719b2daa20",
   "metadata": {},
   "source": [
    "[OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/RAG_with_graph_db.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec732b39-46a7-469b-b73f-185ba0018a25",
   "metadata": {},
   "source": [
    "[Question and Answering Embedding](https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcaf647-e955-4156-ac7b-54ef9043badd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
