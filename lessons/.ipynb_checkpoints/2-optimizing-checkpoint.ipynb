{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "331877dd-1e34-41f4-ad28-c84b4d0c9ce5",
   "metadata": {},
   "source": [
    "# Optimizing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e8ebf-4694-404a-b5f0-fad73877cd54",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8df332-5ac3-47e3-86be-3c00da39cac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f60317f8-83e9-43b7-8ab9-e0c4f6be3991",
   "metadata": {},
   "source": [
    "* Blank empty lines can clutter the text files and make them harder to process. A simple function can remove those lines and tidy up the files.  Then remove the extra spacing and append the modified text to a list. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ba802-52a6-410f-9dd4-bf667adabf1e",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4463fd-384c-4fd6-a4bd-b049d79b4177",
   "metadata": {},
   "source": [
    "* The tokenization process splits the input text into tokens by breaking down the sentences and words. A visual demonstration of this can be seen by checking out our Tokenizer in the docs.\n",
    "\n",
    "* A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly Â¾ of a word (so 100 tokens ~= 75 words).\n",
    "\n",
    "The API has a limit on the maximum number of input tokens for embeddings. To stay below the limit, the text in the CSV file needs to be broken down into multiple rows. The existing length of each row will be recorded first to identify which rows need to be split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f018ee6-2349-435e-b858-2cba4eb0c95d",
   "metadata": {},
   "source": [
    "* The newest embeddings model can handle inputs with up to 8191 input tokens so most of the rows would not need any chunking, but this may not be the case for every subpage scraped so the next code chunk will split the longer lines into smaller chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02077637-e3ff-4cfb-971c-84c81609bd7c",
   "metadata": {},
   "source": [
    "[Web QA Embeddings](https://platform.openai.com/docs/tutorials/web-qa-embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be3a47e-c788-4df7-bab5-cb8c322f8b57",
   "metadata": {},
   "source": [
    "[Embedding Wiki Articles](https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1946f-2b6c-4eba-b559-eec671203f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
